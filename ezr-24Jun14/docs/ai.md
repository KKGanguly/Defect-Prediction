## AI

.

## Notes

- Explanation is really inference. Explanation does not just augment "it", it can replace "it".
- Epsilons are large. If you do enough repeated trials you realize
  how unstable are our conclusions if we (e.g.) runt he whole inference 20
  times, each time using 90% of the day.  So often, minor improvements are spurious
  since (if you do the stats) there is not small set of solutions that
  are statistically indistinguishable from the best solution. so one tactic here
   is to get close, then stop early
  - see the Hamlet equation: big impact on inference
  - note, not for safety critical apps
- Very little is key. much of the signal in just a few variables


-  intuitions version space. hyperspace. hyerspace boundaries. PCA to fastmap.
   explotation vs exploration. 
   - https://medium.com/@mustgofaster/to-explore-or-to-exploit-thats-the-question-dc5152be0468
   generalization error (no self test).
   the chessbpard model (for classification). the snakes and ladders board (for optimization).
   attractors (simliar things have similar properties). y=f(X). g=f(w); y=g(X) .
   feature selection (why should the indenepent variables predict for the dependent).
   instance selection (models come from repeated roww.. so our training data
   can be collapsed to a fee exemplars).
-  a very optimisit canalysis
  -  a story of n gaussians. where do they spend their time? simpksiti analysis
     - hamlet's maths.
       - infinite confidence has infinite cost
       - finite confidence has very little cost
       - finite confidence + binary chop is very cheap
- too optimisistic? perhaps not
- [Entropy](XXX)
- [Mean and standard deviation](xxx)  (and the welford icnremental algorthm)
- Bayes classifiers
- [Decision trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
  - [see also](https://link.springer.com/article/10.1007/s10462-022-10275-5)
- [Discretization](xxx) unsupervised EWD, EFD. supervised entropy, goal-oriented, otehrs in the lirterature 
- [Effect sizes and significance tests](xxx) and sk 
- [Keys](xxx): druzdel's systems, SE results.
- [Semi-supervised learning and Active learning and sequential model optimization](xxx)
- [Distance measures](xxx) Euclidean,Minkowski, Chebyshev, JC's space, distance2heaven , Zitzler, 
- [PCA and Fastmap]
- CLustering, kmeans, rrp (cwunsupervised) , sway (semi-supervised)
- optimization, simulated aannearling, pareto, GAs, maxwalksat, surrogate modelsing, sequaianl model optimization
- Hyper-parameter optimization
- Data synthesis
- Privacy, fairness


